#!/usr/bin/env python3
"""
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼ˆå­¦ç¿’æ™‚æœªä½¿ç”¨ï¼‰ã§è©•ä¾¡ã—ã€çœŸã®æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹
"""

import os
import sys
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Tuple
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.processing.data_loader import KeyboardIntentDataset, create_data_loaders, variable_length_collate_fn
from src.processing.models.hand_models import HandLSTM
from src.processing.models.hand_models import create_model
from torch.utils.data import DataLoader
from config.feature_config import get_feature_dim


def evaluate_on_testset(model_path: str, data_dir: str, 
                       output_dir: str = None):
    """
    ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡
    
    Args:
        model_path: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ.pthãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆdata/trainingï¼‰
        output_dir: çµæœä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    
    å‡¦ç†:
    1. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    2. ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    3. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡
       - Top-1ç²¾åº¦
       - Top-3ç²¾åº¦
       - ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
       - æ··åŒè¡Œåˆ—
    4. çµæœã‚’JSONãƒ»PNGã§ä¿å­˜
    5. ã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º
    """
    
    # ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ç”Ÿæˆ
    if output_dir is None:
        model_dir = os.path.basename(os.path.dirname(model_path))
        output_dir = f'evaluation_results/offline/{model_dir}'
    
    print("ğŸš€ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"   ãƒ¢ãƒ‡ãƒ«: {model_path}")
    print(f"   ãƒ‡ãƒ¼ã‚¿: {data_dir}")
    print(f"   å‡ºåŠ›å…ˆ: {output_dir}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 0. ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆå¯å¤‰é•·å¯¾å¿œã‹ã©ã†ã‹ã‚’ç¢ºèªï¼‰
        checkpoint = torch.load(model_path, map_location='cpu')
        model_config = checkpoint.get('model_config', {})
        use_variable_length = model_config.get('use_variable_length', False)
        
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
        print(f"   å¯å¤‰é•·å¯¾å¿œ: {'æœ‰åŠ¹' if use_variable_length else 'ç„¡åŠ¹'}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆå¯å¤‰é•·å¯¾å¿œï¼‰
        _, _, test_loader = create_data_loaders(
            data_dir=data_dir,
            batch_size=32,
            train_ratio=0.6,
            val_ratio=0.2,
            test_ratio=0.2,
            augment=False,
            use_variable_length=use_variable_length
        )
        
        test_dataset = test_loader.dataset
        
        if len(test_dataset) == 0:
            print("âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return False
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
        
        # 2. ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å†åˆ©ç”¨ï¼‰
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        model = load_model_from_checkpoint(checkpoint, model_config)
        if model is None:
            return False
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {device}")
        
        # 3. è©•ä¾¡ãƒ«ãƒ¼ãƒ—
        print("\nğŸ§ª ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡ä¸­...")
        all_predictions = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                if batch_idx % 10 == 0:
                    print(f"   ãƒãƒƒãƒ {batch_idx + 1}/{len(test_loader)}")
                
                # å¯å¤‰é•·å¯¾å¿œï¼šbatchã¯(features, labels)ã¾ãŸã¯(features, labels, lengths)
                if use_variable_length:
                    features, labels, lengths = batch
                    lengths = lengths.to(device)
                else:
                    features, labels = batch
                    lengths = None
                
                # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                features = features.to(device)
                labels = labels.to(device)
                
                # é †ä¼æ’­
                outputs = model(features, lengths)
                probabilities = torch.softmax(outputs, dim=1)
                
                # Top-1äºˆæ¸¬
                _, predicted = torch.max(outputs, 1)
                
                # è¨˜éŒ²
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probabilities.cpu().numpy())
        
        print("âœ… è©•ä¾¡å®Œäº†")
        
        # 4. ç²¾åº¦è¨ˆç®—
        print("\nğŸ“ˆ ç²¾åº¦ã‚’è¨ˆç®—ä¸­...")
        metrics = calculate_metrics(all_labels, all_predictions, all_probs, test_dataset)
        
        # 5. å¯è¦–åŒ–
        print("\nğŸ¨ å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
        create_visualizations(all_labels, all_predictions, test_dataset, output_dir)
        
        # 6. çµæœä¿å­˜
        print("\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
        save_results(metrics, all_labels, all_predictions, test_dataset, 
                    model_path, data_dir, output_dir)
        
        # 7. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        show_summary(metrics, len(test_dataset), output_dir)
        
        return True
        
    except Exception as e:
        print(f"âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def load_model(model_path: str):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ï¼‰"""
    try:
        checkpoint = torch.load(model_path, map_location='cpu')
        model_config = checkpoint.get('model_config', {})
        return load_model_from_checkpoint(checkpoint, model_config)
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


def load_model_from_checkpoint(checkpoint: dict, model_config: dict):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        input_size = model_config.get('input_size', get_feature_dim())
        hidden_size = model_config.get('hidden_size', 128)
        num_classes = model_config.get('num_classes', 37)
        model_type = model_config.get('model_type', 'lstm')
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆå¯å¤‰é•·å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œï¼‰
        if model_type in ['cnn', 'gru', 'lstm', 'tcn']:
            # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«æ§‹é€ 
            model_params = {
                'model_type': model_type,
                'input_size': input_size,
                'num_classes': num_classes
            }
            
            # GRU/LSTMã®ã¿hidden_sizeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            if model_type in ['gru', 'lstm']:
                model_params['hidden_size'] = hidden_size
            
            model = create_model(**model_params)
        else:
            # å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åã®å ´åˆã€LSTMã¨ã—ã¦æ‰±ã†
            model = HandLSTM(
                input_size=input_size,
                hidden_size=hidden_size,
                num_classes=num_classes
            )
        
        # é‡ã¿èª­ã¿è¾¼ã¿
        model.load_state_dict(checkpoint['model_state_dict'])
        
        return model
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


def calculate_metrics(all_labels: List[int], all_predictions: List[int], 
                     all_probs: List[np.ndarray], test_dataset) -> Dict:
    """ç²¾åº¦æŒ‡æ¨™ã®è¨ˆç®—"""
    
    # Top-1ç²¾åº¦
    top1_accuracy = accuracy_score(all_labels, all_predictions) * 100
    
    # Top-3ç²¾åº¦
    top3_correct = 0
    for i, label in enumerate(all_labels):
        top3_indices = np.argsort(all_probs[i])[-3:]
        if label in top3_indices:
            top3_correct += 1
    top3_accuracy = (top3_correct / len(all_labels)) * 100
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(all_labels, all_predictions)
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
    report = classification_report(
        all_labels, all_predictions,
        target_names=[test_dataset.index_to_key(i) for i in range(37)],
        output_dict=True,
        zero_division=0
    )
    
    return {
        'top1_accuracy': round(top1_accuracy, 2),
        'top3_accuracy': round(top3_accuracy, 2),
        'confusion_matrix': cm,
        'classification_report': report
    }


def create_visualizations(all_labels: List[int], all_predictions: List[int], 
                         test_dataset, output_dir: str):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    
    # æ··åŒè¡Œåˆ—ã®ãƒ—ãƒ­ãƒƒãƒˆ
    cm = confusion_matrix(all_labels, all_predictions)
    
    plt.figure(figsize=(15, 12))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=[test_dataset.index_to_key(i) for i in range(37)],
               yticklabels=[test_dataset.index_to_key(i) for i in range(37)])
    plt.title('Confusion Matrix (Test Set)', fontsize=16)
    plt.xlabel('Predicted', fontsize=12)
    plt.ylabel('Actual', fontsize=12)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ã®ãƒ—ãƒ­ãƒƒãƒˆ
    report = classification_report(
        all_labels, all_predictions,
        target_names=[test_dataset.index_to_key(i) for i in range(37)],
        output_dict=True,
        zero_division=0
    )
    
    class_accuracies = []
    class_names = []
    for i in range(37):
        key = test_dataset.index_to_key(i)
        class_names.append(key)
        acc = report[key]['recall'] * 100 if key in report else 0
        class_accuracies.append(acc)
    
    plt.figure(figsize=(18, 8))
    bars = plt.bar(range(37), class_accuracies, color='skyblue', edgecolor='navy', alpha=0.7)
    
    # å„ãƒãƒ¼ã«æ•°å€¤ã‚’è¡¨ç¤º
    for i, (bar, acc) in enumerate(zip(bars, class_accuracies)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                f'{acc:.1f}%', ha='center', va='bottom', fontsize=8)
    
    plt.xlabel('Class', fontsize=12)
    plt.ylabel('Accuracy (%)', fontsize=12)
    plt.title('Per-Class Accuracy (Test Set)', fontsize=16)
    plt.xticks(range(37), class_names, rotation=45)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/per_class_accuracy.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")


def save_results(metrics: Dict, all_labels: List[int], all_predictions: List[int], 
                test_dataset, model_path: str, data_dir: str, output_dir: str):
    """çµæœã®ä¿å­˜"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # çµæœãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
    results = {
        'timestamp': datetime.now().isoformat(),
        'model_path': model_path,
        'data_dir': data_dir,
        'test_samples': len(test_dataset),
        'metrics': {
            'top1_accuracy': metrics['top1_accuracy'],
            'top3_accuracy': metrics['top3_accuracy']
        },
        'confusion_matrix': metrics['confusion_matrix'].tolist(),
        'classification_report': metrics['classification_report']
    }
    
    # JSONä¿å­˜
    json_path = f'{output_dir}/offline_evaluation_{timestamp}.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… çµæœã‚’ä¿å­˜: {json_path}")


def show_summary(metrics: Dict, test_samples: int, output_dir: str):
    """ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
    
    print("\n" + "="*60)
    print("ğŸ“Š ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡çµæœï¼ˆãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼‰")
    print("="*60)
    print(f"ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {test_samples}")
    print(f"Top-1ç²¾åº¦:        {metrics['top1_accuracy']:.2f}%")
    print(f"Top-3ç²¾åº¦:        {metrics['top3_accuracy']:.2f}%")
    print("="*60)
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ã®ä¸Šä½ãƒ»ä¸‹ä½ã‚’è¡¨ç¤º
    report = metrics['classification_report']
    class_accuracies = []
    for i in range(37):
        key = f"{'a' if i < 26 else '0' if i < 36 else ' '}"
        if key in report:
            class_accuracies.append((key, report[key]['recall'] * 100))
    
    class_accuracies.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ† ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ï¼ˆä¸Šä½5ä½ï¼‰:")
    for i, (key, acc) in enumerate(class_accuracies[:5]):
        print(f"   {i+1}. {key}: {acc:.2f}%")
    
    print(f"\nğŸ“‰ ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ï¼ˆä¸‹ä½5ä½ï¼‰:")
    for i, (key, acc) in enumerate(class_accuracies[-5:]):
        print(f"   {len(class_accuracies)-4+i}. {key}: {acc:.2f}%")
    
    print("="*60)
    print(f"\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   è©³ç´°çµæœ: {output_dir}/offline_evaluation_*.json")
    print(f"   æ··åŒè¡Œåˆ—: {output_dir}/confusion_matrix.png")
    print(f"   ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦: {output_dir}/per_class_accuracy.png")
    print("="*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡')
    parser.add_argument('--model', type=str, required=True,
                        help='ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆ.pthãƒ•ã‚¡ã‚¤ãƒ«ï¼‰')
    parser.add_argument('--data-dir', type=str, default='data/training',
                        help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='çµæœä¿å­˜å…ˆï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ãƒ¢ãƒ‡ãƒ«åã§è‡ªå‹•ç”Ÿæˆï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.model):
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.model}")
        return False
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.data_dir):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.data_dir}")
        return False
    
    # è©•ä¾¡å®Ÿè¡Œ
    success = evaluate_on_testset(
        model_path=args.model,
        data_dir=args.data_dir,
        output_dir=args.output_dir
    )
    
    if success:
        print("\nâœ… ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        return True
    else:
        print("\nâŒ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        return False


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Ÿè·µçš„ãªå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦LSTMãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
"""

import os
import argparse
import json
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# æ—¢å­˜ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append('src')
from src.processing.data_loader import create_data_loaders, KeyboardIntentDataset
from src.processing.models.hand_lstm import BasicHandLSTM


class IntentModelTrainer:
    """æ„å›³æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir: str, epochs: int = 100, batch_size: int = 32,
                 learning_rate: float = 0.001, model_save_path: str = None):
        """
        å­¦ç¿’ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
            model_save_path: ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        self.data_dir = data_dir
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆã®è¨­å®š
        if model_save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.model_save_path = f"models/intent_model_{timestamp}"
        else:
            self.model_save_path = model_save_path
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs(self.model_save_path, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.train_loader = None
        self.val_loader = None
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        
        # å­¦ç¿’å±¥æ­´
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.train_top3_accuracies = []
        self.val_top3_accuracies = []
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=f"runs/intent_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        
        # Early Stopping
        self.best_val_loss = float('inf')
        self.patience = 5
        self.patience_counter = 0
        
        print(f"ğŸ¯ æ„å›³æ¨å®šãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        print(f"   å­¦ç¿’ç‡: {learning_rate}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.model_save_path}")
    
    def setup_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®šä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
            self.train_loader, self.val_loader = create_data_loaders(
                data_dir=self.data_dir,
                batch_size=self.batch_size,
                augment=True,
                num_workers=0
            )
            
            if len(self.train_loader) == 0 or len(self.val_loader) == 0:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã§ã™")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’å–å¾—
            train_dataset = self.train_loader.dataset
            val_dataset = self.val_loader.dataset
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šå®Œäº†")
            print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
            print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
            print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {train_dataset.feature_dim}")
            print(f"   æ™‚ç³»åˆ—é•·: {train_dataset.sequence_length}")
            print(f"   ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®š"""
        print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            train_dataset = self.train_loader.dataset
            
            # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            self.model = BasicHandLSTM(
                input_size=train_dataset.feature_dim,
                hidden_size=128,
                num_classes=train_dataset.num_classes
            ).to(self.device)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
            print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
            print(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
            
            # æå¤±é–¢æ•°
            self.criterion = nn.CrossEntropyLoss()
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
            
            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='min', factor=0.5, patience=3, verbose=True
            )
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def train_epoch(self, epoch: int):
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
        self.model.train()
        total_loss = 0.0
        correct_predictions = 0
        correct_top3_predictions = 0
        total_samples = 0
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¡¨ç¤º
        num_batches = len(self.train_loader)
        
        for batch_idx, (features, labels) in enumerate(self.train_loader):
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            features = features.to(self.device)
            labels = labels.to(self.device)
            
            # å‹¾é…ã‚’ã‚¼ãƒ­ã«ãƒªã‚»ãƒƒãƒˆ
            self.optimizer.zero_grad()
            
            # é †ä¼æ’­
            outputs = self.model(features)
            loss = self.criterion(outputs, labels)
            
            # é€†ä¼æ’­
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            self.optimizer.step()
            
            # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
            total_loss += loss.item()
            total_samples += labels.size(0)
            
            # ç²¾åº¦ã®è¨ˆç®—
            _, predicted = torch.max(outputs.data, 1)
            correct_predictions += (predicted == labels).sum().item()
            
            # Top-3ç²¾åº¦ã®è¨ˆç®—
            _, top3_indices = torch.topk(outputs.data, 3, dim=1)
            correct_top3 = torch.sum(top3_indices == labels.unsqueeze(1), dim=1)
            correct_top3_predictions += correct_top3.sum().item()
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°
            if batch_idx % 10 == 0:
                progress = (batch_idx + 1) / num_batches
                bar_length = 20
                filled_length = int(bar_length * progress)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                print(f"\rEpoch {epoch}/{self.epochs} [{bar}] {progress*100:.1f}%", end='')
        
        print()  # æ”¹è¡Œ
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—
        avg_loss = total_loss / num_batches
        accuracy = (correct_predictions / total_samples) * 100
        top3_accuracy = (correct_top3_predictions / total_samples) * 100
        
        return avg_loss, accuracy, top3_accuracy
    
    def validate_epoch(self, epoch: int):
        """1ã‚¨ãƒãƒƒã‚¯ã®æ¤œè¨¼"""
        self.model.eval()
        total_loss = 0.0
        correct_predictions = 0
        correct_top3_predictions = 0
        total_samples = 0
        
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for features, labels in self.val_loader:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                # é †ä¼æ’­
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                
                # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
                total_loss += loss.item()
                total_samples += labels.size(0)
                
                # ç²¾åº¦ã®è¨ˆç®—
                _, predicted = torch.max(outputs.data, 1)
                correct_predictions += (predicted == labels).sum().item()
                
                # Top-3ç²¾åº¦ã®è¨ˆç®—
                _, top3_indices = torch.topk(outputs.data, 3, dim=1)
                correct_top3 = torch.sum(top3_indices == labels.unsqueeze(1), dim=1)
                correct_top3_predictions += correct_top3.sum().item()
                
                # æ··åŒè¡Œåˆ—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—
        avg_loss = total_loss / len(self.val_loader)
        accuracy = (correct_predictions / total_samples) * 100
        top3_accuracy = (correct_top3_predictions / total_samples) * 100
        
        return avg_loss, accuracy, top3_accuracy, all_predictions, all_labels
    
    def train(self):
        """å­¦ç¿’ã®å®Ÿè¡Œ"""
        print("\nğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        print("=" * 60)
        
        # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        if not self.setup_data() or not self.setup_model():
            print("âŒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # å­¦ç¿’é–‹å§‹æ™‚åˆ»
        start_time = time.time()
        
        print(f"\nğŸ“Š å­¦ç¿’è¨­å®š:")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.train_loader.dataset) + len(self.val_loader.dataset)}")
        print(f"   è¨“ç·´/æ¤œè¨¼åˆ†å‰²: {len(self.train_loader.dataset)}/{len(self.val_loader.dataset)}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print("=" * 60)
        
        try:
            for epoch in range(1, self.epochs + 1):
                epoch_start_time = time.time()
                
                # è¨“ç·´
                train_loss, train_acc, train_top3_acc = self.train_epoch(epoch)
                
                # æ¤œè¨¼
                val_loss, val_acc, val_top3_acc, predictions, labels = self.validate_epoch(epoch)
                
                # å­¦ç¿’å±¥æ­´ã®ä¿å­˜
                self.train_losses.append(train_loss)
                self.val_losses.append(val_loss)
                self.train_accuracies.append(train_acc)
                self.val_accuracies.append(val_acc)
                self.train_top3_accuracies.append(train_top3_acc)
                self.val_top3_accuracies.append(val_top3_acc)
                
                # TensorBoardã¸ã®è¨˜éŒ²
                self.writer.add_scalar('Loss/Train', train_loss, epoch)
                self.writer.add_scalar('Loss/Validation', val_loss, epoch)
                self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
                self.writer.add_scalar('Accuracy/Validation', val_acc, epoch)
                self.writer.add_scalar('Top3_Accuracy/Train', train_top3_acc, epoch)
                self.writer.add_scalar('Top3_Accuracy/Validation', val_top3_acc, epoch)
                
                # ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã®è¨ˆç®—
                epoch_time = time.time() - epoch_start_time
                
                # çµæœã®è¡¨ç¤º
                print(f"Epoch {epoch:3d}/{self.epochs} - "
                      f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                      f"Val Acc: {val_acc:.1f}%, Top-3: {val_top3_acc:.1f}% "
                      f"({epoch_time:.1f}s)")
                
                # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®æ›´æ–°
                self.scheduler.step(val_loss)
                
                # Early Stopping
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.patience_counter = 0
                    
                    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
                    self.save_model("best_model.pth", epoch, val_loss, val_acc)
                    print(f"ğŸ’¾ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ (Val Loss: {val_loss:.4f})")
                else:
                    self.patience_counter += 1
                    if self.patience_counter >= self.patience:
                        print(f"â¹ï¸  Early Stopping: {self.patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                        break
                
                # å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜
                if epoch % 10 == 0:
                    self.save_model(f"checkpoint_epoch_{epoch}.pth", epoch, val_loss, val_acc)
            
            # å­¦ç¿’å®Œäº†
            total_time = time.time() - start_time
            print(f"\nâœ… å­¦ç¿’å®Œäº†ï¼")
            print(f"   ç·å­¦ç¿’æ™‚é–“: {total_time/60:.1f}åˆ†")
            print(f"   æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_acc:.1f}%")
            print(f"   æœ€çµ‚æ¤œè¨¼Top-3ç²¾åº¦: {val_top3_acc:.1f}%")
            
            # æœ€çµ‚çµæœã®ä¿å­˜
            self.save_final_results(predictions, labels)
            
            return True
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ å­¦ç¿’ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            return False
        except Exception as e:
            print(f"\nâŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            self.cleanup()
    
    def save_model(self, filename: str, epoch: int, val_loss: float, val_acc: float):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        filepath = os.path.join(self.model_save_path, filename)
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'val_accuracy': val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'train_top3_accuracies': self.train_top3_accuracies,
            'val_top3_accuracies': self.val_top3_accuracies,
            'model_config': {
                'input_size': self.train_loader.dataset.feature_dim,
                'hidden_size': 128,
                'num_classes': self.train_loader.dataset.num_classes,
                'sequence_length': self.train_loader.dataset.sequence_length
            },
            'training_config': {
                'epochs': self.epochs,
                'batch_size': self.batch_size,
                'learning_rate': self.learning_rate,
                'data_dir': self.data_dir
            }
        }, filepath)
    
    def save_final_results(self, predictions: list, labels: list):
        """æœ€çµ‚çµæœã®ä¿å­˜"""
        print("ğŸ’¾ æœ€çµ‚çµæœã‚’ä¿å­˜ä¸­...")
        
        # æ··åŒè¡Œåˆ—ã®è¨ˆç®—
        cm = confusion_matrix(labels, predictions)
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        dataset = self.val_loader.dataset
        target_names = [dataset.index_to_key(i) for i in range(dataset.num_classes)]
        report = classification_report(labels, predictions, target_names=target_names, output_dict=True)
        
        # çµæœã®ä¿å­˜
        results = {
            'final_metrics': {
                'final_val_loss': self.val_losses[-1] if self.val_losses else None,
                'final_val_accuracy': self.val_accuracies[-1] if self.val_accuracies else None,
                'final_val_top3_accuracy': self.val_top3_accuracies[-1] if self.val_top3_accuracies else None,
                'best_val_loss': self.best_val_loss,
                'total_epochs': len(self.train_losses)
            },
            'confusion_matrix': cm.tolist(),
            'classification_report': report,
            'learning_history': {
                'train_losses': self.train_losses,
                'val_losses': self.val_losses,
                'train_accuracies': self.train_accuracies,
                'val_accuracies': self.val_accuracies,
                'train_top3_accuracies': self.train_top3_accuracies,
                'val_top3_accuracies': self.val_top3_accuracies
            }
        }
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        results_file = os.path.join(self.model_save_path, 'training_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–
        self.plot_confusion_matrix(cm, target_names)
        
        # å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
        self.plot_learning_curves()
        
        print(f"âœ… æœ€çµ‚çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
    
    def plot_confusion_matrix(self, cm: np.ndarray, target_names: list):
        """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–"""
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=target_names, yticklabels=target_names)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜
        cm_file = os.path.join(self.model_save_path, 'confusion_matrix.png')
        plt.savefig(cm_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {cm_file}")
    
    def plot_learning_curves(self):
        """å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # æå¤±æ›²ç·š
        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')
        ax1.plot(epochs, self.val_losses, 'r-', label='Validation Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # ç²¾åº¦æ›²ç·š
        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')
        ax2.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        # Top-3ç²¾åº¦æ›²ç·š
        ax3.plot(epochs, self.train_top3_accuracies, 'b-', label='Training Top-3 Accuracy')
        ax3.plot(epochs, self.val_top3_accuracies, 'r-', label='Validation Top-3 Accuracy')
        ax3.set_title('Training and Validation Top-3 Accuracy')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Top-3 Accuracy (%)')
        ax3.legend()
        ax3.grid(True)
        
        # å­¦ç¿’ç‡ã®æ¨ç§»
        ax4.plot(epochs, [self.learning_rate] * len(epochs), 'g-', label='Learning Rate')
        ax4.set_title('Learning Rate')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Learning Rate')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜
        curves_file = os.path.join(self.model_save_path, 'learning_curves.png')
        plt.savefig(curves_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ: {curves_file}")
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.writer:
            self.writer.close()
        print("ğŸ§¹ ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='æ„å›³æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--data-dir', default='data/training/user_001', 
                       help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: data/training/user_001)')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='ã‚¨ãƒãƒƒã‚¯æ•° (default: 100)')
    parser.add_argument('--batch-size', type=int, default=32, 
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 32)')
    parser.add_argument('--learning-rate', type=float, default=0.001, 
                       help='å­¦ç¿’ç‡ (default: 0.001)')
    parser.add_argument('--model-save-path', default=None, 
                       help='ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹ (default: auto-generated)')
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.data_dir):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.data_dir}")
        print("ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return 1
    
    print("ğŸ¯ Training Intent Prediction Model")
    print("=" * 50)
    
    # å­¦ç¿’ã‚¯ãƒ©ã‚¹ã®ä½œæˆã¨å®Ÿè¡Œ
    trainer = IntentModelTrainer(
        data_dir=args.data_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        model_save_path=args.model_save_path
    )
    
    # å­¦ç¿’ã®å®Ÿè¡Œ
    success = trainer.train()
    
    if success:
        print("\nğŸ‰ å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:")
        print(f"   {trainer.model_save_path}")
        print(f"ğŸŒ TensorBoardãƒ­ã‚°: {trainer.writer.log_dir}")
        return 0
    else:
        print("\nâŒ å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return 1


if __name__ == "__main__":
    exit(main())

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Ÿè·µçš„ãªå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦LSTMãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
"""

import os
import argparse
import json
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# æ—¢å­˜ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append('src')
from src.processing.data_loader import create_data_loaders, KeyboardIntentDataset
from src.processing.models.hand_lstm import BasicHandLSTM


class IntentModelTrainer:
    """æ„å›³æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir: str, epochs: int = 100, batch_size: int = 32,
                 learning_rate: float = 0.001, model_save_path: str = None):
        """
        å­¦ç¿’ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
            model_save_path: ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        self.data_dir = data_dir
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆã®è¨­å®š
        if model_save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.model_save_path = f"models/intent_model_{timestamp}"
        else:
            self.model_save_path = model_save_path
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs(self.model_save_path, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.train_loader = None
        self.val_loader = None
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        
        # å­¦ç¿’å±¥æ­´
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.train_top3_accuracies = []
        self.val_top3_accuracies = []
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=f"runs/intent_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        
        # Early Stopping
        self.best_val_loss = float('inf')
        self.patience = 5
        self.patience_counter = 0
        
        print(f"ğŸ¯ æ„å›³æ¨å®šãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        print(f"   å­¦ç¿’ç‡: {learning_rate}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.model_save_path}")
    
    def setup_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®šä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆï¼ˆ3åˆ†å‰²å¯¾å¿œï¼‰
            self.train_loader, self.val_loader, self.test_loader = create_data_loaders(
                data_dir=self.data_dir,
                batch_size=self.batch_size,
                augment=True,
                num_workers=0
            )
            
            if len(self.train_loader) == 0:
                raise ValueError("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã§ã™")
            
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆã¯è­¦å‘Š
            if len(self.val_loader) == 0:
                print("âš ï¸ è­¦å‘Š: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            if len(self.test_loader) == 0:
                print("âš ï¸ è­¦å‘Š: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚æœ€çµ‚è©•ä¾¡ãŒã§ãã¾ã›ã‚“")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’å–å¾—
            train_dataset = self.train_loader.dataset
            val_dataset = self.val_loader.dataset
            test_dataset = self.test_loader.dataset
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šå®Œäº†")
            print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
            print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
            print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
            print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {train_dataset.feature_dim}")
            print(f"   æ™‚ç³»åˆ—é•·: {train_dataset.sequence_length}")
            print(f"   ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®š"""
        print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            train_dataset = self.train_loader.dataset
            
            # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            self.model = BasicHandLSTM(
                input_size=train_dataset.feature_dim,
                hidden_size=128,
                num_classes=train_dataset.num_classes
            ).to(self.device)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
            print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
            print(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
            
            # æå¤±é–¢æ•°ï¼ˆã‚¯ãƒ©ã‚¹é‡ã¿å¯¾å¿œï¼‰
            class_weights = train_dataset.get_class_weights()
            self.criterion = nn.CrossEntropyLoss(weight=class_weights.to(self.device))
            print(f"âœ… ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’ä½¿ç”¨ã—ãŸæå¤±é–¢æ•°ã‚’è¨­å®šã—ã¾ã—ãŸ")
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
            
            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='min', factor=0.5, patience=3, verbose=True
            )
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def train_epoch(self, epoch: int):
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
        self.model.train()
        total_loss = 0.0
        correct_predictions = 0
        correct_top3_predictions = 0
        total_samples = 0
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¡¨ç¤º
        num_batches = len(self.train_loader)
        
        for batch_idx, (features, labels) in enumerate(self.train_loader):
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            features = features.to(self.device)
            labels = labels.to(self.device)
            
            # å‹¾é…ã‚’ã‚¼ãƒ­ã«ãƒªã‚»ãƒƒãƒˆ
            self.optimizer.zero_grad()
            
            # é †ä¼æ’­
            outputs = self.model(features)
            loss = self.criterion(outputs, labels)
            
            # é€†ä¼æ’­
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            self.optimizer.step()
            
            # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
            total_loss += loss.item()
            total_samples += labels.size(0)
            
            # ç²¾åº¦ã®è¨ˆç®—
            _, predicted = torch.max(outputs.data, 1)
            correct_predictions += (predicted == labels).sum().item()
            
            # Top-3ç²¾åº¦ã®è¨ˆç®—
            _, top3_indices = torch.topk(outputs.data, 3, dim=1)
            correct_top3 = torch.sum(top3_indices == labels.unsqueeze(1), dim=1)
            correct_top3_predictions += correct_top3.sum().item()
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°
            if batch_idx % 10 == 0:
                progress = (batch_idx + 1) / num_batches
                bar_length = 20
                filled_length = int(bar_length * progress)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                print(f"\rEpoch {epoch}/{self.epochs} [{bar}] {progress*100:.1f}%", end='')
        
        print()  # æ”¹è¡Œ
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—
        avg_loss = total_loss / num_batches
        accuracy = (correct_predictions / total_samples) * 100
        top3_accuracy = (correct_top3_predictions / total_samples) * 100
        
        return avg_loss, accuracy, top3_accuracy
    
    def validate_epoch(self, epoch: int):
        """1ã‚¨ãƒãƒƒã‚¯ã®æ¤œè¨¼"""
        self.model.eval()
        total_loss = 0.0
        correct_predictions = 0
        correct_top3_predictions = 0
        total_samples = 0
        
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for features, labels in self.val_loader:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                # é †ä¼æ’­
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                
                # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
                total_loss += loss.item()
                total_samples += labels.size(0)
                
                # ç²¾åº¦ã®è¨ˆç®—
                _, predicted = torch.max(outputs.data, 1)
                correct_predictions += (predicted == labels).sum().item()
                
                # Top-3ç²¾åº¦ã®è¨ˆç®—
                _, top3_indices = torch.topk(outputs.data, 3, dim=1)
                correct_top3 = torch.sum(top3_indices == labels.unsqueeze(1), dim=1)
                correct_top3_predictions += correct_top3.sum().item()
                
                # æ··åŒè¡Œåˆ—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—
        avg_loss = total_loss / len(self.val_loader)
        accuracy = (correct_predictions / total_samples) * 100
        top3_accuracy = (correct_top3_predictions / total_samples) * 100
        
        return avg_loss, accuracy, top3_accuracy, all_predictions, all_labels
    
    def train(self):
        """å­¦ç¿’ã®å®Ÿè¡Œ"""
        print("\nğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        print("=" * 60)
        
        # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        if not self.setup_data() or not self.setup_model():
            print("âŒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # å­¦ç¿’é–‹å§‹æ™‚åˆ»
        start_time = time.time()
        
        print(f"\nğŸ“Š å­¦ç¿’è¨­å®š:")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.train_loader.dataset) + len(self.val_loader.dataset)}")
        print(f"   è¨“ç·´/æ¤œè¨¼åˆ†å‰²: {len(self.train_loader.dataset)}/{len(self.val_loader.dataset)}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print("=" * 60)
        
        try:
            for epoch in range(1, self.epochs + 1):
                epoch_start_time = time.time()
                
                # è¨“ç·´
                train_loss, train_acc, train_top3_acc = self.train_epoch(epoch)
                
                # æ¤œè¨¼
                val_loss, val_acc, val_top3_acc, predictions, labels = self.validate_epoch(epoch)
                
                # å­¦ç¿’å±¥æ­´ã®ä¿å­˜
                self.train_losses.append(train_loss)
                self.val_losses.append(val_loss)
                self.train_accuracies.append(train_acc)
                self.val_accuracies.append(val_acc)
                self.train_top3_accuracies.append(train_top3_acc)
                self.val_top3_accuracies.append(val_top3_acc)
                
                # TensorBoardã¸ã®è¨˜éŒ²
                self.writer.add_scalar('Loss/Train', train_loss, epoch)
                self.writer.add_scalar('Loss/Validation', val_loss, epoch)
                self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
                self.writer.add_scalar('Accuracy/Validation', val_acc, epoch)
                self.writer.add_scalar('Top3_Accuracy/Train', train_top3_acc, epoch)
                self.writer.add_scalar('Top3_Accuracy/Validation', val_top3_acc, epoch)
                
                # ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã®è¨ˆç®—
                epoch_time = time.time() - epoch_start_time
                
                # çµæœã®è¡¨ç¤º
                print(f"Epoch {epoch:3d}/{self.epochs} - "
                      f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                      f"Val Acc: {val_acc:.1f}%, Top-3: {val_top3_acc:.1f}% "
                      f"({epoch_time:.1f}s)")
                
                # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®æ›´æ–°
                self.scheduler.step(val_loss)
                
                # Early Stopping
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.patience_counter = 0
                    
                    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
                    self.save_model("best_model.pth", epoch, val_loss, val_acc)
                    print(f"ğŸ’¾ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ (Val Loss: {val_loss:.4f})")
                else:
                    self.patience_counter += 1
                    if self.patience_counter >= self.patience:
                        print(f"â¹ï¸  Early Stopping: {self.patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                        break
                
                # å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜
                if epoch % 10 == 0:
                    self.save_model(f"checkpoint_epoch_{epoch}.pth", epoch, val_loss, val_acc)
            
            # å­¦ç¿’å®Œäº†
            total_time = time.time() - start_time
            print(f"\nâœ… å­¦ç¿’å®Œäº†ï¼")
            print(f"   ç·å­¦ç¿’æ™‚é–“: {total_time/60:.1f}åˆ†")
            print(f"   æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_acc:.1f}%")
            print(f"   æœ€çµ‚æ¤œè¨¼Top-3ç²¾åº¦: {val_top3_acc:.1f}%")
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡
            test_results = self.evaluate_test_data()
            
            # æœ€çµ‚çµæœã®ä¿å­˜
            self.save_final_results(predictions, labels, test_results)
            
            return True
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ å­¦ç¿’ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            return False
        except Exception as e:
            print(f"\nâŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            self.cleanup()
    
    def evaluate_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡"""
        print("\nğŸ§ª ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
        
        if len(self.test_loader) == 0:
            print("âš ï¸ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return None
        
        self.model.eval()
        test_loss = 0.0
        correct = 0
        top3_correct = 0
        total = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.test_loader):
                if torch.cuda.is_available():
                    data, target = data.cuda(), target.cuda()
                
                output = self.model(data)
                loss = self.criterion(output, target)
                test_loss += loss.item()
                
                # äºˆæ¸¬ã®è¨ˆç®—
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                
                # Top-3ç²¾åº¦ã®è¨ˆç®—
                _, top3_pred = output.topk(3, dim=1)
                top3_correct += top3_pred.eq(target.view(-1, 1)).sum().item()
                
                total += target.size(0)
                
                # äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜
                all_predictions.extend(pred.cpu().numpy().flatten())
                all_labels.extend(target.cpu().numpy())
        
        # å¹³å‡æå¤±ã¨ç²¾åº¦ã‚’è¨ˆç®—
        avg_test_loss = test_loss / len(self.test_loader)
        test_accuracy = 100. * correct / total
        test_top3_accuracy = 100. * top3_correct / total
        
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡çµæœ:")
        print(f"   ãƒ†ã‚¹ãƒˆæå¤±: {avg_test_loss:.4f}")
        print(f"   ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.2f}%")
        print(f"   Top-3ç²¾åº¦: {test_top3_accuracy:.2f}%")
        
        # çµæœã‚’ä¿å­˜
        test_results = {
            'test_loss': avg_test_loss,
            'test_accuracy': test_accuracy,
            'test_top3_accuracy': test_top3_accuracy,
            'predictions': all_predictions,
            'labels': all_labels
        }
        
        return test_results
    
    def save_model(self, filename: str, epoch: int, val_loss: float, val_acc: float):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        filepath = os.path.join(self.model_save_path, filename)
        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚­ãƒ¼ã®å¯¾å¿œï¼‰
        train_dataset = self.train_loader.dataset
        label_map = list(train_dataset.KEY_CHARS)
        # ä½µã›ã¦JSONã¨ã—ã¦ã‚‚ä¿å­˜ï¼ˆå¯æ¬æ€§å‘ä¸Šï¼‰
        label_map_file = os.path.join(self.model_save_path, 'label_map.json')
        try:
            with open(label_map_file, 'w', encoding='utf-8') as f:
                json.dump({'labels': label_map}, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ label_map.json ã®ä¿å­˜ã«å¤±æ•—: {e}")
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'val_accuracy': val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'train_top3_accuracies': self.train_top3_accuracies,
            'val_top3_accuracies': self.val_top3_accuracies,
            'model_config': {
                'input_size': train_dataset.feature_dim,
                'hidden_size': 128,
                'num_classes': train_dataset.num_classes,
                'sequence_length': train_dataset.sequence_length
            },
            'training_config': {
                'epochs': self.epochs,
                'batch_size': self.batch_size,
                'learning_rate': self.learning_rate,
                'data_dir': self.data_dir
            },
            'label_map': label_map
        }, filepath)
    
    def save_final_results(self, predictions: list, labels: list, test_results: dict = None):
        """æœ€çµ‚çµæœã®ä¿å­˜"""
        print("ğŸ’¾ æœ€çµ‚çµæœã‚’ä¿å­˜ä¸­...")
        
        # NumPyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›ï¼ˆJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
        predictions = [int(p) for p in predictions]
        labels = [int(l) for l in labels]
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        val_dataset = self.val_loader.dataset
        
        # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚’ç‰¹å®šï¼ˆäºˆæ¸¬ã¾ãŸã¯æ­£è§£ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã®ã¿ï¼‰
        unique_labels = sorted(list(set(labels + predictions)))
        target_names = [val_dataset.index_to_key(i) for i in unique_labels]

        # å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ··åŒè¡Œåˆ—ã‚’è¨ˆç®—
        cm = confusion_matrix(labels, predictions, labels=unique_labels)
        
        report = classification_report(labels, predictions, labels=unique_labels, target_names=target_names, output_dict=True, zero_division=0)
        
        # test_resultsã®NumPyå‹ã‚‚å¤‰æ›
        if test_results and 'predictions' in test_results:
            test_results['predictions'] = [int(p) for p in test_results['predictions']]
        if test_results and 'labels' in test_results:
            test_results['labels'] = [int(l) for l in test_results['labels']]
        
        # çµæœã®ä¿å­˜
        results = {
            'final_metrics': {
                'final_val_loss': float(self.val_losses[-1]) if self.val_losses else None,
                'final_val_accuracy': float(self.val_accuracies[-1]) if self.val_accuracies else None,
                'final_val_top3_accuracy': float(self.val_top3_accuracies[-1]) if self.val_top3_accuracies else None,
                'best_val_loss': float(self.best_val_loss),
                'total_epochs': int(len(self.train_losses))
            },
            'test_metrics': test_results if test_results else None,
            'confusion_matrix': cm.tolist(),
            'classification_report': report,
            'learning_history': {
                'train_losses': [float(loss) for loss in self.train_losses],
                'val_losses': [float(loss) for loss in self.val_losses],
                'train_accuracies': [float(acc) for acc in self.train_accuracies],
                'val_accuracies': [float(acc) for acc in self.val_accuracies],
                'train_top3_accuracies': [float(acc) for acc in self.train_top3_accuracies],
                'val_top3_accuracies': [float(acc) for acc in self.val_top3_accuracies]
            }
        }
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        results_file = os.path.join(self.model_save_path, 'training_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–
        self.plot_confusion_matrix(cm, target_names)
        
        # å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
        self.plot_learning_curves()
        
        print(f"âœ… æœ€çµ‚çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
    
    
    def plot_confusion_matrix(self, cm: np.ndarray, target_names: list):
        """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–"""
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=target_names, yticklabels=target_names)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜
        cm_file = os.path.join(self.model_save_path, 'confusion_matrix.png')
        plt.savefig(cm_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {cm_file}")
    
    def plot_learning_curves(self):
        """å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # æå¤±æ›²ç·š
        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')
        ax1.plot(epochs, self.val_losses, 'r-', label='Validation Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # ç²¾åº¦æ›²ç·š
        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')
        ax2.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        # Top-3ç²¾åº¦æ›²ç·š
        ax3.plot(epochs, self.train_top3_accuracies, 'b-', label='Training Top-3 Accuracy')
        ax3.plot(epochs, self.val_top3_accuracies, 'r-', label='Validation Top-3 Accuracy')
        ax3.set_title('Training and Validation Top-3 Accuracy')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Top-3 Accuracy (%)')
        ax3.legend()
        ax3.grid(True)
        
        # å­¦ç¿’ç‡ã®æ¨ç§»
        ax4.plot(epochs, [self.learning_rate] * len(epochs), 'g-', label='Learning Rate')
        ax4.set_title('Learning Rate')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Learning Rate')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜
        curves_file = os.path.join(self.model_save_path, 'learning_curves.png')
        plt.savefig(curves_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ: {curves_file}")
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.writer:
            self.writer.close()
        print("ğŸ§¹ ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='æ„å›³æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--data-dir', default='data/training/user_001', 
                       help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: data/training/user_001)')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='ã‚¨ãƒãƒƒã‚¯æ•° (default: 100)')
    parser.add_argument('--batch-size', type=int, default=32, 
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 32)')
    parser.add_argument('--learning-rate', type=float, default=0.001, 
                       help='å­¦ç¿’ç‡ (default: 0.001)')
    parser.add_argument('--model-save-path', default=None, 
                       help='ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹ (default: auto-generated)')
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.data_dir):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.data_dir}")
        print("ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return 1
    
    print("ğŸ¯ Training Intent Prediction Model")
    print("=" * 50)
    
    # å­¦ç¿’ã‚¯ãƒ©ã‚¹ã®ä½œæˆã¨å®Ÿè¡Œ
    trainer = IntentModelTrainer(
        data_dir=args.data_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        model_save_path=args.model_save_path
    )
    
    # å­¦ç¿’ã®å®Ÿè¡Œ
    success = trainer.train()
    
    if success:
        print("\nğŸ‰ å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:")
        print(f"   {trainer.model_save_path}")
        print(f"ğŸŒ TensorBoardãƒ­ã‚°: {trainer.writer.log_dir}")
        return 0
    else:
        print("\nâŒ å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return 1


if __name__ == "__main__":
    exit(main())

# src/processing/models/hand_models.py
"""
å„ç¨®æ‰‹ã®è»Œè·¡èªè­˜ãƒ¢ãƒ‡ãƒ«
å¯å¤‰é•·å¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from config.feature_config import get_feature_dim, get_num_classes


# ====================================
# 1. 1D-CNNï¼ˆæ¨å¥¨ï¼‰
# ====================================
class HandCNN(nn.Module):
    """
    1D-CNNãƒ¢ãƒ‡ãƒ«ï¼ˆå¯å¤‰é•·å¯¾å¿œãŒç°¡å˜ï¼‰
    
    ç•³ã¿è¾¼ã¿ã§ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé€Ÿåº¦ã€åŠ é€Ÿåº¦ï¼‰ã‚’æ‰ãˆã€
    AdaptivePoolingã§å¯å¤‰é•·ã‚’å›ºå®šé•·ã«å¤‰æ›ã™ã‚‹ã€‚
    """
    
    def __init__(self, input_size=None, num_classes=None, dropout=0.2):
        super().__init__()
        self.input_size = input_size or get_feature_dim()
        self.num_classes = num_classes or get_num_classes()
        
        # ç•³ã¿è¾¼ã¿å±¤
        self.conv1 = nn.Conv1d(self.input_size, 64, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(64)
        
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        
        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm1d(256)
        
        # å¯å¤‰é•·ã‚’å›ºå®šé•·ã«å¤‰æ›ï¼ˆé‡è¦ï¼ï¼‰
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(dropout)
        
        # å…¨çµåˆå±¤
        self.fc = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, self.num_classes)
        )
    
    def forward(self, x, lengths=None):
        """
        Forward pass
        
        Args:
            x: (batch, seq_len, input_size)
            lengths: (batch,) - å„ç³»åˆ—ã®å®Ÿéš›ã®é•·ã•ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
        
        Returns:
            output: (batch, num_classes)
        """
        # (batch, seq_len, input_size) â†’ (batch, input_size, seq_len)
        x = x.transpose(1, 2)
        
        # ç•³ã¿è¾¼ã¿
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.dropout(x)
        
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.dropout(x)
        
        # å¯å¤‰é•·ã‚’1ã«é›†ç´„ï¼ˆlengthsä¸è¦ï¼ï¼‰
        x = self.adaptive_pool(x).squeeze(-1)
        
        # å…¨çµåˆ
        output = self.fc(x)
        
        return output


# ====================================
# 2. GRUï¼ˆLSTMã‚ˆã‚Šé«˜é€Ÿï¼‰
# ====================================
class HandGRU(nn.Module):
    """
    GRUãƒ¢ãƒ‡ãƒ«ï¼ˆLSTMã®æ”¹è‰¯ç‰ˆï¼‰
    
    LSTMã‚ˆã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãé«˜é€Ÿã€‚
    åŒç­‰ã®ç²¾åº¦ãŒæœŸå¾…ã§ãã‚‹ã€‚
    """
    
    def __init__(self, input_size=None, hidden_size=128, num_layers=2, 
                 num_classes=None, dropout=0.2):
        super().__init__()
        self.input_size = input_size or get_feature_dim()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes or get_num_classes()
        
        # GRUå±¤
        self.gru = nn.GRU(
            self.input_size,
            self.hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # å…¨çµåˆå±¤
        self.fc = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_size // 2, self.num_classes)
        )
    
    def forward(self, x, lengths=None):
        """
        Forward pass
        
        Args:
            x: (batch, seq_len, input_size)
            lengths: (batch,) - å„ç³»åˆ—ã®å®Ÿéš›ã®é•·ã•
        
        Returns:
            output: (batch, num_classes)
        """
        if lengths is not None:
            # å¯å¤‰é•·å¯¾å¿œï¼ˆPackedSequenceï¼‰
            packed = pack_padded_sequence(
                x, lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            _, hidden = self.gru(packed)
        else:
            # å›ºå®šé•·ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
            _, hidden = self.gru(x)
        
        # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ï¼ˆæœ€ä¸Šå±¤ï¼‰
        output = self.fc(hidden[-1])
        
        return output


# ====================================
# 3. LSTMï¼ˆå¯å¤‰é•·å¯¾å¿œç‰ˆï¼‰
# ====================================
class HandLSTM(nn.Module):
    """
    LSTMãƒ¢ãƒ‡ãƒ«ï¼ˆå¯å¤‰é•·å¯¾å¿œç‰ˆï¼‰
    
    ç¾åœ¨ã®BasicHandLSTMã‚’å¯å¤‰é•·å¯¾å¿œã«æ”¹è‰¯ã—ãŸã‚‚ã®ã€‚
    """
    
    def __init__(self, input_size=None, hidden_size=128, num_layers=2,
                 num_classes=None, dropout=0.2):
        super().__init__()
        self.input_size = input_size or get_feature_dim()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes or get_num_classes()
        
        # LSTMå±¤
        self.lstm = nn.LSTM(
            self.input_size,
            self.hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # å…¨çµåˆå±¤
        self.fc = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_size // 2, self.num_classes)
        )
    
    def forward(self, x, lengths=None):
        """
        Forward pass
        
        Args:
            x: (batch, seq_len, input_size)
            lengths: (batch,) - å„ç³»åˆ—ã®å®Ÿéš›ã®é•·ã•
        
        Returns:
            output: (batch, num_classes)
        """
        if lengths is not None:
            # å¯å¤‰é•·å¯¾å¿œï¼ˆPackedSequenceï¼‰
            packed = pack_padded_sequence(
                x, lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            _, (hidden, cell) = self.lstm(packed)
        else:
            # å›ºå®šé•·ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
            _, (hidden, cell) = self.lstm(x)
        
        # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ï¼ˆæœ€ä¸Šå±¤ï¼‰
        output = self.fc(hidden[-1])
        
        return output


# ====================================
# 4. TCNï¼ˆTemporal Convolutional Networkï¼‰
# ====================================
class TemporalBlock(nn.Module):
    """TCNã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):
        super().__init__()
        padding = (kernel_size - 1) * dilation
        
        self.conv1 = nn.Conv1d(
            in_channels, out_channels, kernel_size,
            padding=padding, dilation=dilation
        )
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        
        # æ®‹å·®æ¥ç¶šç”¨
        self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿æŒ
        self.padding = padding
    
    def forward(self, x):
        # ç•³ã¿è¾¼ã¿
        out = self.conv1(x)
        
        # å› æœçš„ç•³ã¿è¾¼ã¿ï¼ˆæœªæ¥ã‚’è¦‹ãªã„ï¼‰
        if self.padding > 0:
            out = out[:, :, :-self.padding]
        
        out = self.bn1(out)
        out = self.relu(out)
        out = self.dropout(out)
        
        # æ®‹å·®æ¥ç¶š
        if self.residual is not None:
            res = self.residual(x)
        else:
            res = x
        
        # æ®‹å·®æ¥ç¶šã®ã‚µã‚¤ã‚ºèª¿æ•´
        if res.shape[2] != out.shape[2]:
            res = res[:, :, :out.shape[2]]
        
        return self.relu(out + res)


class HandTCN(nn.Module):
    """
    TCNãƒ¢ãƒ‡ãƒ«ï¼ˆTemporal Convolutional Networkï¼‰
    
    å› æœçš„ç•³ã¿è¾¼ã¿ã¨dilationã§é•·æœŸä¾å­˜ã‚’æ‰ãˆã‚‹ã€‚
    RNNã‚ˆã‚Šä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã§é«˜é€Ÿã€‚
    """
    
    def __init__(self, input_size=None, num_channels=[64, 128, 256], 
                 kernel_size=3, num_classes=None, dropout=0.2):
        super().__init__()
        self.input_size = input_size or get_feature_dim()
        self.num_classes = num_classes or get_num_classes()
        
        # TCNå±¤
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            in_channels = self.input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            dilation = 2 ** i  # æŒ‡æ•°çš„ã«dilationã‚’å¢—ã‚„ã™
            
            layers.append(
                TemporalBlock(in_channels, out_channels, kernel_size, dilation, dropout)
            )
        
        self.network = nn.Sequential(*layers)
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        
        # å…¨çµåˆå±¤
        self.fc = nn.Sequential(
            nn.Linear(num_channels[-1], 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, self.num_classes)
        )
    
    def forward(self, x, lengths=None):
        """
        Forward pass
        
        Args:
            x: (batch, seq_len, input_size)
            lengths: (batch,) - å„ç³»åˆ—ã®å®Ÿéš›ã®é•·ã•ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
        
        Returns:
            output: (batch, num_classes)
        """
        # (batch, seq_len, input_size) â†’ (batch, input_size, seq_len)
        x = x.transpose(1, 2)
        
        # TCNå‡¦ç†
        x = self.network(x)
        
        # å¯å¤‰é•·ã‚’1ã«é›†ç´„
        x = self.adaptive_pool(x).squeeze(-1)
        
        # å…¨çµåˆ
        output = self.fc(x)
        
        return output


# ====================================
# ãƒ¢ãƒ‡ãƒ«ä½œæˆé–¢æ•°
# ====================================
def create_model(model_type='cnn', **kwargs):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹
    
    Args:
        model_type: 'cnn', 'gru', 'lstm', 'tcn'
        **kwargs: ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    
    Examples:
        >>> model = create_model('cnn', input_size=18, num_classes=37)
        >>> model = create_model('gru', hidden_size=128, num_layers=2)
        >>> model = create_model('lstm', dropout=0.3)
        >>> model = create_model('tcn', num_channels=[64, 128, 256])
    """
    model_type = model_type.lower()
    
    if model_type == 'cnn':
        return HandCNN(**kwargs)
    elif model_type == 'gru':
        return HandGRU(**kwargs)
    elif model_type == 'lstm':
        return HandLSTM(**kwargs)
    elif model_type == 'tcn':
        return HandTCN(**kwargs)
    else:
        raise ValueError(f"Unknown model type: {model_type}. Choose from: cnn, gru, lstm, tcn")


def get_model_info(model_type='cnn'):
    """
    ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹
    
    Args:
        model_type: 'cnn', 'gru', 'lstm', 'tcn'
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¾æ›¸
    """
    info = {
        'cnn': {
            'name': '1D-CNN',
            'description': 'ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯',
            'pros': ['é«˜é€Ÿ', 'å¯å¤‰é•·å¯¾å¿œãŒç°¡å˜', 'ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¼·ã„'],
            'cons': ['é•·æœŸä¾å­˜ã¯é™å®šçš„'],
            'best_for': 'é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ãªã©ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜'
        },
        'gru': {
            'name': 'GRU',
            'description': 'Gated Recurrent Unit',
            'pros': ['LSTMã‚ˆã‚Šé«˜é€Ÿ', 'LSTMã‚ˆã‚Šçœãƒ¡ãƒ¢ãƒª', 'åŒç­‰ã®ç²¾åº¦'],
            'cons': ['å­¦ç¿’ãŒé€æ¬¡çš„'],
            'best_for': 'LSTMã®ä»£æ›¿ã¨ã—ã¦'
        },
        'lstm': {
            'name': 'LSTM',
            'description': 'Long Short-Term Memory',
            'pros': ['é•·æœŸä¾å­˜ã‚’å­¦ç¿’', 'å®Ÿè£…ãŒæˆç†Ÿ'],
            'cons': ['å­¦ç¿’ãŒé…ã„', 'ãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¤§'],
            'best_for': 'ç³»åˆ—å…¨ä½“ã®æ–‡è„ˆãŒå¿…è¦ãªå ´åˆ'
        },
        'tcn': {
            'name': 'TCN',
            'description': 'Temporal Convolutional Network',
            'pros': ['ä¸¦åˆ—è¨ˆç®—å¯èƒ½', 'é•·æœŸä¾å­˜ã‚’æ‰ãˆã‚‹', 'æœ€æ–°æ‰‹æ³•'],
            'cons': ['å®Ÿè£…ãŒã‚„ã‚„è¤‡é›‘'],
            'best_for': 'RNNã®ä»£æ›¿ã¨ã—ã¦ï¼ˆé«˜é€Ÿç‰ˆï¼‰'
        }
    }
    
    return info.get(model_type.lower(), None)


# ====================================
# ãƒ†ã‚¹ãƒˆ
# ====================================
if __name__ == "__main__":
    print("ğŸ§ª ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    batch_size = 8
    input_size = 18
    num_classes = 37
    
    # å¯å¤‰é•·ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    lengths = torch.tensor([15, 30, 45, 60, 20, 35, 50, 25])
    max_length = lengths.max().item()
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    x = torch.randn(batch_size, max_length, input_size)
    
    print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {x.shape}")
    print(f"é•·ã•: {lengths}")
    print()
    
    # å„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    models = ['cnn', 'gru', 'lstm', 'tcn']
    
    for model_type in models:
        print(f"ğŸ“¦ {model_type.upper()} ãƒ¢ãƒ‡ãƒ«")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = create_model(model_type, input_size=input_size, num_classes=num_classes)
        model.eval()
        
        # æ¨è«–
        with torch.no_grad():
            output = model(x, lengths)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        num_params = sum(p.numel() for p in model.parameters())
        
        print(f"   å‡ºåŠ›: {output.shape}")
        print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:,}")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        info = get_model_info(model_type)
        if info:
            print(f"   ç‰¹å¾´: {', '.join(info['pros'])}")
        
        print()
    
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")


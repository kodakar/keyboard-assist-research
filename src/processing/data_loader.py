# src/processing/data_loader.py
"""
PyTorchç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›æ„å›³æ¨å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
"""

import os
import json
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Tuple, Optional
from collections import Counter
import random
from sklearn.model_selection import train_test_split
import warnings
from .feature_extractor import FeatureExtractor


class KeyboardIntentDataset(Dataset):
    """
    ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›æ„å›³æ¨å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
    PyTorchã®Datasetã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿
    """
    
    # 37ã‚­ãƒ¼ã®å®šç¾©ï¼ˆè‹±å­—26å€‹ + æ•°å­—10å€‹ + ã‚¹ãƒšãƒ¼ã‚¹ï¼‰
    KEY_CHARS = (
        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' '
    )
    
    def __init__(self, data_dir: str, sequence_length: int = 60, 
                 train: bool = True, train_ratio: float = 0.8,
                 augment: bool = False, noise_std: float = 0.01,
                 random_seed: int = 42):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            sequence_length: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰
            train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            augment: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¡Œã†ã‹ã©ã†ã‹
            noise_std: ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã®æ¨™æº–åå·®
            random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.data_dir = data_dir
        self.sequence_length = sequence_length
        self.train = train
        self.train_ratio = train_ratio
        self.augment = augment
        self.noise_std = noise_std
        
        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
        random.seed(random_seed)
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        self.samples = self._load_data_files()
        
        # è¨“ç·´/æ¤œè¨¼ã®åˆ†å‰²
        self.samples = self._split_train_val()
        
        # ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°
        self.feature_dim = 15  # 2 + 6 + 3 + 4 = 15æ¬¡å…ƒ
        
        # ã‚¯ãƒ©ã‚¹æ•°
        self.num_classes = len(self.KEY_CHARS)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.samples)}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {self.feature_dim}")
        print(f"   æ™‚ç³»åˆ—é•·: {sequence_length}")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: {self.num_classes}")
        print(f"   ãƒ¢ãƒ¼ãƒ‰: {'è¨“ç·´' if train else 'æ¤œè¨¼'}")
        print(f"   ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: {'æœ‰åŠ¹' if augment else 'ç„¡åŠ¹'}")
    
    def _load_data_files(self) -> List[Dict]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        samples = []
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
            for root, dirs, files in os.walk(self.data_dir):
                for file in files:
                    if file.endswith('.json') and 'sample_' in file:
                        file_path = os.path.join(root, file)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                sample_data = json.load(f)
                                
                                # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                if self._validate_sample(sample_data):
                                    samples.append(sample_data)
                                
                        except Exception as e:
                            warnings.warn(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                            continue
            
            print(f"ğŸ“ {len(samples)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
        return samples
    
    def _validate_sample(self, sample_data: Dict) -> bool:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        required_fields = ['target_char', 'trajectory_data', 'coordinate_system']
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        for field in required_fields:
            if field not in sample_data:
                return False
        
        # åº§æ¨™ç³»ã®ãƒã‚§ãƒƒã‚¯
        if sample_data.get('coordinate_system') != 'relative_keyboard_space':
            return False
        
        # è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        trajectory_data = sample_data.get('trajectory_data', [])
        if not isinstance(trajectory_data, list) or len(trajectory_data) == 0:
            return False
        
        # ç›®æ¨™æ–‡å­—ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        target_char = sample_data.get('target_char', '').lower()
        if target_char not in self.KEY_CHARS:
            return False
        
        return True
    
    def _split_train_val(self) -> List[Dict]:
        """è¨“ç·´/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"""
        if len(self.samples) == 0:
            return []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        user_groups = {}
        for sample in self.samples:
            user_id = sample.get('user_id', 'unknown')
            if user_id not in user_groups:
                user_groups[user_id] = []
            user_groups[user_id].append(sample)
        
        # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«è¨“ç·´/æ¤œè¨¼åˆ†å‰²
        train_samples = []
        val_samples = []
        
        for user_id, user_samples in user_groups.items():
            if len(user_samples) < 2:
                # ã‚µãƒ³ãƒ—ãƒ«ãŒå°‘ãªã„å ´åˆã¯å…¨ä»¶ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«
                train_samples.extend(user_samples)
            else:
                # è¨“ç·´/æ¤œè¨¼åˆ†å‰²
                user_train, user_val = train_test_split(
                    user_samples, 
                    train_size=self.train_ratio, 
                    random_state=42,
                    stratify=[s.get('target_char', '').lower() for s in user_samples]
                )
                train_samples.extend(user_train)
                val_samples.extend(user_val)
        
        # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿”ã™
        if self.train:
            return train_samples
        else:
            return val_samples
    
    def __len__(self) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’è¿”ã™"""
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.FloatTensor, int]:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
        
        Args:
            idx: ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        Returns:
            features: ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ« (sequence_length, feature_dim)
            label: ãƒ©ãƒ™ãƒ«ï¼ˆ0-36ã®æ•´æ•°ï¼‰
        """
        sample = self.samples[idx]
        
        # ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆå…±é€šæŠ½å‡ºå™¨ï¼‰
        if not hasattr(self, '_feature_extractor'):
            self._feature_extractor = FeatureExtractor(sequence_length=self.sequence_length, fps=30.0)
        features_np = self._feature_extractor.extract_from_trajectory(sample.get('trajectory_data', []))
        features = torch.FloatTensor(features_np)
        
        # ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        target_char = sample.get('target_char', '').lower()
        label = self.key_to_index(target_char)
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰
        if self.train and self.augment:
            features = self._augment_features(features)
        
        return features, label
    
    def _extract_features(self, sample: Dict) -> torch.FloatTensor:
        """å¾Œæ–¹äº’æ›ã®ãŸã‚æ®‹ã™ï¼ˆå†…éƒ¨ã§FeatureExtractorã‚’å‘¼ã¶ï¼‰"""
        if not hasattr(self, '_feature_extractor'):
            self._feature_extractor = FeatureExtractor(sequence_length=self.sequence_length, fps=30.0)
        features_np = self._feature_extractor.extract_from_trajectory(sample.get('trajectory_data', []))
        return torch.FloatTensor(features_np)
    
    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """éæ¨å¥¨ï¼ˆFeatureExtractorã«ç§»ç®¡æ¸ˆã¿ï¼‰ã€‚äº’æ›ã®ãŸã‚æ®‹ç½®ã€‚"""
        return features
    
    def _augment_features(self, features: torch.FloatTensor) -> torch.FloatTensor:
        """ç‰¹å¾´é‡ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
        features = features.clone()
        
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã®è¿½åŠ 
        noise = torch.randn_like(features) * self.noise_std
        features = features + noise
        
        # æ™‚é–“è»¸ã®ã‚ãšã‹ãªä¼¸ç¸®ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«1-2ãƒ•ãƒ¬ãƒ¼ãƒ å‰Šé™¤ã¾ãŸã¯è¿½åŠ ï¼‰
        if random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§å®Ÿè¡Œ
            if random.random() < 0.5 and features.shape[0] > 2:
                # ãƒ©ãƒ³ãƒ€ãƒ ã«1ãƒ•ãƒ¬ãƒ¼ãƒ å‰Šé™¤
                remove_idx = random.randint(0, features.shape[0] - 1)
                features = torch.cat([features[:remove_idx], features[remove_idx+1:]])
            else:
                # ãƒ©ãƒ³ãƒ€ãƒ ã«1ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¤‡è£½
                duplicate_idx = random.randint(0, features.shape[0] - 1)
                duplicate_frame = features[duplicate_idx:duplicate_idx+1]
                features = torch.cat([features[:duplicate_idx+1], duplicate_frame, features[duplicate_idx+1:]])
        
        # åº§æ¨™ã®å¾®å°ãªå¹³è¡Œç§»å‹•
        if random.random() < 0.2:  # 20%ã®ç¢ºç‡ã§å®Ÿè¡Œ
            shift = torch.randn(2) * 0.01  # å¾®å°ãªç§»å‹•
            features[:, :2] = features[:, :2] + shift.unsqueeze(0)
        
        return features
    
    def key_to_index(self, key: str) -> int:
        """ã‚­ãƒ¼æ–‡å­—ã‚’0-36ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›"""
        key = key.lower()
        if key in self.KEY_CHARS:
            return self.KEY_CHARS.index(key)
        else:
            # ä¸æ˜ãªã‚­ãƒ¼ã®å ´åˆã¯0ã‚’è¿”ã™
            warnings.warn(f"ä¸æ˜ãªã‚­ãƒ¼: {key}, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0ã‚’ä½¿ç”¨")
            return 0
    
    def index_to_key(self, index: int) -> str:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ¼æ–‡å­—ã«å¤‰æ›"""
        if 0 <= index < len(self.KEY_CHARS):
            return self.KEY_CHARS[index]
        else:
            # ç¯„å›²å¤–ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã¯'a'ã‚’è¿”ã™
            warnings.warn(f"ç¯„å›²å¤–ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index}, ã‚­ãƒ¼'a'ã‚’ä½¿ç”¨")
            return 'a'
    
    def get_label_distribution(self) -> Dict[str, int]:
        """å„ã‚­ãƒ¼ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¿”ã™"""
        label_counts = Counter()
        
        for sample in self.samples:
            target_char = sample.get('target_char', '').lower()
            if target_char in self.KEY_CHARS:
                label_counts[target_char] += 1
        
        return dict(label_counts)
    
    def get_class_weights(self) -> torch.FloatTensor:
        """ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç”¨ï¼‰"""
        label_dist = self.get_label_distribution()
        
        if not label_dist:
            return torch.ones(self.num_classes)
        
        # å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å–å¾—
        class_counts = []
        for key in self.KEY_CHARS:
            count = label_dist.get(key, 1)  # æœ€ä½1ã¯ä¿è¨¼
            class_counts.append(count)
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã®é€†æ•°ï¼‰
        total_samples = sum(class_counts)
        class_weights = [total_samples / (self.num_classes * count) for count in class_counts]
        
        return torch.FloatTensor(class_weights)
    
    def get_sample_info(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’å–å¾—"""
        label_dist = self.get_label_distribution()
        
        return {
            'total_samples': len(self.samples),
            'sequence_length': self.sequence_length,
            'feature_dim': self.feature_dim,
            'num_classes': self.num_classes,
            'label_distribution': label_dist,
            'mode': 'train' if self.train else 'validation',
            'augmentation': self.augment
        }


def create_data_loaders(data_dir: str, batch_size: int = 32, 
                       sequence_length: int = 60, train_ratio: float = 0.8,
                       augment: bool = True, num_workers: int = 0,
                       random_seed: int = 42) -> Tuple[DataLoader, DataLoader]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    
    Args:
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        sequence_length: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·ã•
        train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
        augment: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¡Œã†ã‹ã©ã†ã‹
        num_workers: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        
    Returns:
        train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        val_loader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    """
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_dataset = KeyboardIntentDataset(
        data_dir=data_dir,
        sequence_length=sequence_length,
        train=True,
        train_ratio=train_ratio,
        augment=augment,
        random_seed=random_seed
    )
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    val_dataset = KeyboardIntentDataset(
        data_dir=data_dir,
        sequence_length=sequence_length,
        train=False,
        train_ratio=train_ratio,
        augment=False,  # æ¤œè¨¼æ™‚ã¯æ‹¡å¼µãªã—
        random_seed=random_seed
    )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†")
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    
    return train_loader, val_loader


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚¹ãƒˆ
    data_dir = "data/training/user_001"
    
    if os.path.exists(data_dir):
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        dataset = KeyboardIntentDataset(data_dir, augment=True)
        
        # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã®è¡¨ç¤º
        info = dataset.get_sample_info()
        print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¡¨ç¤º
        class_weights = dataset.get_class_weights()
        print(f"ã‚¯ãƒ©ã‚¹é‡ã¿: {class_weights[:10]}...")  # æœ€åˆã®10å€‹
        
        # ã‚µãƒ³ãƒ—ãƒ«ã®å–å¾—ãƒ†ã‚¹ãƒˆ
        if len(dataset) > 0:
            features, label = dataset[0]
            print(f"ã‚µãƒ³ãƒ—ãƒ«0:")
            print(f"  ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")
            print(f"  ãƒ©ãƒ™ãƒ«: {label} ({dataset.index_to_key(label)})")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆãƒ†ã‚¹ãƒˆ
        train_loader, val_loader = create_data_loaders(data_dir, batch_size=16)
        
    else:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data_dir}")
        print("ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")

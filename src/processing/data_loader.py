# src/processing/data_loader.py
"""
PyTorchç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›æ„å›³æ¨å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
"""

import os
import json
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Tuple, Optional
from collections import Counter
import random
from sklearn.model_selection import train_test_split
import warnings
from .feature_extractor import FeatureExtractor


class KeyboardIntentDataset(Dataset):
    """
    ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›æ„å›³æ¨å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
    PyTorchã®Datasetã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿
    """
    
    # 37ã‚­ãƒ¼ã®å®šç¾©ï¼ˆè‹±å­—26å€‹ + æ•°å­—10å€‹ + ã‚¹ãƒšãƒ¼ã‚¹ï¼‰
    KEY_CHARS = (
        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' '
    )
    
    def __init__(self, data_dir: str, sequence_length: int = 60, 
                 train: bool = True, train_ratio: float = 0.8,
                 augment: bool = False, noise_std: float = 0.01,
                 random_seed: int = 42):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            sequence_length: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰
            train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            augment: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¡Œã†ã‹ã©ã†ã‹
            noise_std: ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã®æ¨™æº–åå·®
            random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.data_dir = data_dir
        self.sequence_length = sequence_length
        self.train = train
        self.train_ratio = train_ratio
        self.augment = augment
        self.noise_std = noise_std
        
        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
        random.seed(random_seed)
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        self.samples = self._load_data_files()
        
        # è¨“ç·´/æ¤œè¨¼ã®åˆ†å‰²
        self.samples = self._split_train_val()
        
        # ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°ï¼ˆå›ºå®šï¼‰
        self.feature_dim = 18
        
        # ç‰¹å¾´é‡æŠ½å‡ºå™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
        self.feature_extractor = FeatureExtractor(sequence_length=self.sequence_length)
        
        # ã‚¯ãƒ©ã‚¹æ•°
        self.num_classes = len(self.KEY_CHARS)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.samples)}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {self.feature_dim}")
        print(f"   æ™‚ç³»åˆ—é•·: {sequence_length}")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: {self.num_classes}")
        print(f"   ãƒ¢ãƒ¼ãƒ‰: {'è¨“ç·´' if train else 'æ¤œè¨¼'}")
        print(f"   ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: {'æœ‰åŠ¹' if augment else 'ç„¡åŠ¹'}")
    
    def _load_data_files(self) -> List[Dict]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        samples = []
        
        try:
            print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢ä¸­: {self.data_dir}")
            # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
            for root, dirs, files in os.walk(self.data_dir):
                print(f"ğŸ“ æ¢ç´¢ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {root}")
                print(f"ğŸ“ ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {dirs}")
                print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {files}")
                for file in files:
                    print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯: {file} (JSON: {file.endswith('.json')}, sample_å«ã‚€: {'sample_' in file})")
                    if file.endswith('.json') and 'sample_' in file:
                        file_path = os.path.join(root, file)
                        print(f"ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {file_path}")
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                sample_data = json.load(f)
                                print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {file_path}")
                                
                                # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                if self._validate_sample(sample_data):
                                    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼æˆåŠŸ: {file_path}")
                                    samples.append(sample_data)
                                else:
                                    print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼å¤±æ•—: {file_path}")
                                
                        except Exception as e:
                            warnings.warn(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                            continue
            
            print(f"ğŸ“ {len(samples)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
        return samples
    
    def _validate_sample(self, sample_data: Dict) -> bool:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        required_fields = ['target_char', 'trajectory_data', 'coordinate_system']
        
        print(f"ğŸ” ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼é–‹å§‹: {list(sample_data.keys())}")
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        for field in required_fields:
            if field not in sample_data:
                print(f"âŒ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸è¶³: {field}")
                return False
        
        # åº§æ¨™ç³»ã®ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°ã—ã„åº§æ¨™ç³»ã‚‚å—ã‘å…¥ã‚Œã‚‹ï¼‰
        coord_sys = sample_data.get('coordinate_system')
        print(f"ğŸ” åº§æ¨™ç³»: {coord_sys}")
        if coord_sys not in ['relative_keyboard_space', 'work_area_v2']:
            print(f"âŒ åº§æ¨™ç³»ä¸ä¸€è‡´: {coord_sys} (æœŸå¾…å€¤: relative_keyboard_space ã¾ãŸã¯ work_area_v2)")
            return False
        
        # è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        trajectory_data = sample_data.get('trajectory_data', [])
        print(f"ğŸ” è»Œè·¡ãƒ‡ãƒ¼ã‚¿é•·: {len(trajectory_data)}")
        if not isinstance(trajectory_data, list) or len(trajectory_data) == 0:
            print(f"âŒ è»Œè·¡ãƒ‡ãƒ¼ã‚¿ä¸æ­£: {type(trajectory_data)}, é•·ã•: {len(trajectory_data) if isinstance(trajectory_data, list) else 'N/A'}")
            return False
        
        # ç›®æ¨™æ–‡å­—ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        target_char = sample_data.get('target_char', '').lower()
        print(f"ğŸ” ç›®æ¨™æ–‡å­—: {target_char}")
        if target_char not in self.KEY_CHARS:
            print(f"âŒ ç›®æ¨™æ–‡å­—ä¸æ­£: {target_char}")
            return False
        
        print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼æˆåŠŸ")
        return True
    
    def _split_train_val(self) -> List[Dict]:
        """è¨“ç·´/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"""
        if len(self.samples) == 0:
            return []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        user_groups = {}
        for sample in self.samples:
            user_id = sample.get('user_id', 'unknown')
            if user_id not in user_groups:
                user_groups[user_id] = []
            user_groups[user_id].append(sample)
        
        # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«è¨“ç·´/æ¤œè¨¼åˆ†å‰²
        train_samples = []
        val_samples = []
        
        for user_id, user_samples in user_groups.items():
            if len(user_samples) < 2:
                # ã‚µãƒ³ãƒ—ãƒ«ãŒå°‘ãªã„å ´åˆã¯å…¨ä»¶ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«
                train_samples.extend(user_samples)
                print(f"   ğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id}: ã‚µãƒ³ãƒ—ãƒ«æ•° {len(user_samples)} < 2 ã®ãŸã‚å…¨ä»¶ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«")
            else:
                # è¨“ç·´/æ¤œè¨¼åˆ†å‰²ï¼ˆã‚¯ãƒ©ã‚¹æ•°ãŒå°‘ãªã„å ´åˆã¯stratifyã‚’ç„¡åŠ¹åŒ–ï¼‰
                try:
                    user_train, user_val = train_test_split(
                        user_samples, 
                        train_size=self.train_ratio, 
                        random_state=42,
                        stratify=[s.get('target_char', '').lower() for s in user_samples]
                    )
                except ValueError as e:
                    # stratifyã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆï¼ˆã‚¯ãƒ©ã‚¹æ•°ãŒå°‘ãªã„å ´åˆï¼‰
                    print(f"   âš ï¸ stratifyåˆ†å‰²ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    print(f"   ğŸ“ é€šå¸¸åˆ†å‰²ã‚’ä½¿ç”¨ã—ã¾ã™")
                    user_train, user_val = train_test_split(
                        user_samples, 
                        train_size=self.train_ratio, 
                        random_state=42
                    )
                train_samples.extend(user_train)
                val_samples.extend(user_val)
                print(f"   ğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id}: è¨“ç·´ {len(user_train)}, æ¤œè¨¼ {len(user_val)}")
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if len(train_samples) == 0:
            print(f"   âš ï¸ è­¦å‘Š: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™")
            return []
        
        if len(val_samples) == 0:
            print(f"   âš ï¸ è­¦å‘Š: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã‚‚ä½¿ç”¨ã—ã¾ã™")
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨
            if len(train_samples) >= 2:
                val_samples = train_samples[:1]  # æœ€åˆã®1ä»¶ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«
                train_samples = train_samples[1:]  # æ®‹ã‚Šã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«
                print(f"   ğŸ“ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ: è¨“ç·´ {len(train_samples)}, æ¤œè¨¼ {len(val_samples)}")
        
        # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿”ã™
        if self.train:
            return train_samples
        else:
            return val_samples
    
    def __len__(self) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’è¿”ã™"""
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.FloatTensor, int]:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
        
        Args:
            idx: ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        Returns:
            features: ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ« (sequence_length, feature_dim)
            label: ãƒ©ãƒ™ãƒ«ï¼ˆ0-36ã®æ•´æ•°ï¼‰
        """
        sample = self.samples[idx]
        
        # è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
        trajectory = sample.get('trajectory_data', [])
        if not self.feature_extractor:
            self.feature_extractor = FeatureExtractor(sequence_length=self.sequence_length)
        
        # è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’çµ±ä¸€
        trajectory = self._normalize_trajectory_length(trajectory)
        
        features_np = self.feature_extractor.extract_from_trajectory(trajectory)
        features = torch.FloatTensor(features_np)
        
        # ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        target_char = sample.get('target_char', '').lower()
        label = self.key_to_index(target_char)
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰
        if self.train and self.augment:
            features = self._augment_features(features)
        
        return features, label
    
    def _extract_features(self, sample: Dict) -> torch.FloatTensor:
        """å¾Œæ–¹äº’æ›ã®ãŸã‚æ®‹ã™ï¼ˆå†…éƒ¨ã§FeatureExtractorã‚’å‘¼ã¶ï¼‰"""
        if not self.feature_extractor:
            self.feature_extractor = FeatureExtractor(sequence_length=self.sequence_length)
        features_np = self.feature_extractor.extract_from_trajectory(sample.get('trajectory_data', []))
        return torch.FloatTensor(features_np)
    
    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """éæ¨å¥¨ï¼ˆFeatureExtractorã«ç§»ç®¡æ¸ˆã¿ï¼‰ã€‚äº’æ›ã®ãŸã‚æ®‹ç½®ã€‚"""
        return features
    
    def _normalize_trajectory_length(self, trajectory: List[Dict]) -> List[Dict]:
        """è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’sequence_lengthã«çµ±ä¸€"""
        if len(trajectory) == self.sequence_length:
            return trajectory
        elif len(trajectory) > self.sequence_length:
            # é•·ã™ãã‚‹å ´åˆã¯ä¸­å¤®éƒ¨åˆ†ã‚’æŠ½å‡º
            start_idx = (len(trajectory) - self.sequence_length) // 2
            return trajectory[start_idx:start_idx + self.sequence_length]
        else:
            # çŸ­ã™ãã‚‹å ´åˆã¯æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¹°ã‚Šè¿”ã—
            normalized = trajectory.copy()
            last_frame = trajectory[-1] if trajectory else {}
            while len(normalized) < self.sequence_length:
                normalized.append(last_frame)
            return normalized
    
    def _augment_features(self, features: torch.FloatTensor) -> torch.FloatTensor:
        """ç‰¹å¾´é‡ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
        features = features.clone()
        
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã®è¿½åŠ 
        noise = torch.randn_like(features) * self.noise_std
        features = features + noise
        
        # åº§æ¨™ã®å¾®å°ãªå¹³è¡Œç§»å‹•
        if random.random() < 0.2:  # 20%ã®ç¢ºç‡ã§å®Ÿè¡Œ
            shift = torch.randn(2) * 0.01  # å¾®å°ãªç§»å‹•
            features[:, :2] = features[:, :2] + shift.unsqueeze(0)
        
        # äººå·¥çš„ãªéœ‡ãˆã®è¿½åŠ ï¼ˆ50%ã®ç¢ºç‡ã§å®Ÿè¡Œï¼‰
        if random.random() < 0.5:
            features = self._add_artificial_tremor(features)
        
        # å½¢çŠ¶ã®ä¿è¨¼
        assert features.shape == (self.sequence_length, self.feature_dim), \
            f"ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå¾Œã®å½¢çŠ¶ãŒä¸æ­£: {features.shape}, æœŸå¾…: ({self.sequence_length}, {self.feature_dim})"
        
        return features
    
    def _add_artificial_tremor(self, features: torch.FloatTensor) -> torch.FloatTensor:
        """äººå·¥çš„ãªéœ‡ãˆã‚’ç‰¹å¾´é‡ã«è¿½åŠ """
        features = features.clone()
        
        # åŸºæœ¬ã¨ãªã‚‹æ­£å¼¦æ³¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ±ºå®š
        frequency = random.uniform(4.0, 12.0)  # 4Hzã‹ã‚‰12Hz
        amplitude_x = random.uniform(0.005, 0.02)  # æŒ¯å¹…0.005ã‹ã‚‰0.02
        amplitude_y = amplitude_x * 0.8  # yæ–¹å‘ã¯xæ–¹å‘ã®80%ã®å¼·ã•
        
        # æ™‚é–“è»¸ã®ç”Ÿæˆï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«åŸºã¥ãï¼‰
        time_steps = torch.arange(self.sequence_length, dtype=torch.float32)
        
        # æ­£å¼¦æ³¢ã®ç”Ÿæˆ
        tremor_x = amplitude_x * torch.sin(2 * torch.pi * frequency * time_steps / self.sequence_length)
        tremor_y = amplitude_y * torch.sin(2 * torch.pi * frequency * time_steps / self.sequence_length)
        
        # ä¸è¦å‰‡æ€§ã‚’åŠ ãˆã‚‹ï¼ˆæ­£è¦åˆ†å¸ƒãƒã‚¤ã‚ºã€æŒ¯å¹…ã®10%ç¨‹åº¦ï¼‰
        noise_x = torch.randn(self.sequence_length) * amplitude_x * 0.1
        noise_y = torch.randn(self.sequence_length) * amplitude_y * 0.1
        
        # éœ‡ãˆãƒ‡ãƒ¼ã‚¿ã‚’åº§æ¨™ã«åŠ ç®—
        features[:, 0] = features[:, 0] + tremor_x + noise_x  # xåº§æ¨™ï¼ˆ0åˆ—ç›®ï¼‰
        features[:, 1] = features[:, 1] + tremor_y + noise_y  # yåº§æ¨™ï¼ˆ1åˆ—ç›®ï¼‰
        
        # åº§æ¨™ãŒ0.0ã‹ã‚‰1.0ã®ç¯„å›²ã«åã¾ã‚‹ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        features[:, :2] = torch.clamp(features[:, :2], 0.0, 1.0)
        
        return features
    
    def key_to_index(self, key: str) -> int:
        """ã‚­ãƒ¼æ–‡å­—ã‚’0-36ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›"""
        key = key.lower()
        if key in self.KEY_CHARS:
            return self.KEY_CHARS.index(key)
        else:
            # ä¸æ˜ãªã‚­ãƒ¼ã®å ´åˆã¯0ã‚’è¿”ã™
            warnings.warn(f"ä¸æ˜ãªã‚­ãƒ¼: {key}, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0ã‚’ä½¿ç”¨")
            return 0
    
    def index_to_key(self, index: int) -> str:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ¼æ–‡å­—ã«å¤‰æ›"""
        if 0 <= index < len(self.KEY_CHARS):
            return self.KEY_CHARS[index]
        else:
            # ç¯„å›²å¤–ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã¯'a'ã‚’è¿”ã™
            warnings.warn(f"ç¯„å›²å¤–ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index}, ã‚­ãƒ¼'a'ã‚’ä½¿ç”¨")
            return 'a'
    
    def get_label_distribution(self) -> Dict[str, int]:
        """å„ã‚­ãƒ¼ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¿”ã™"""
        label_counts = Counter()
        
        for sample in self.samples:
            target_char = sample.get('target_char', '').lower()
            if target_char in self.KEY_CHARS:
                label_counts[target_char] += 1
        
        return dict(label_counts)
    
    def get_class_weights(self) -> torch.FloatTensor:
        """ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç”¨ï¼‰"""
        label_dist = self.get_label_distribution()
        
        if not label_dist:
            return torch.ones(self.num_classes)
        
        # å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å–å¾—
        class_counts = []
        for key in self.KEY_CHARS:
            count = label_dist.get(key, 1)  # æœ€ä½1ã¯ä¿è¨¼
            class_counts.append(count)
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã®é€†æ•°ï¼‰
        total_samples = sum(class_counts)
        class_weights = [total_samples / (self.num_classes * count) for count in class_counts]
        
        return torch.FloatTensor(class_weights)
    
    def get_sample_info(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’å–å¾—"""
        label_dist = self.get_label_distribution()
        
        return {
            'total_samples': len(self.samples),
            'sequence_length': self.sequence_length,
            'feature_dim': self.feature_dim,
            'num_classes': self.num_classes,
            'label_distribution': label_dist,
            'mode': 'train' if self.train else 'validation',
            'augmentation': self.augment
        }


def create_data_loaders(data_dir: str, batch_size: int = 32, 
                       sequence_length: int = 60, train_ratio: float = 0.8,
                       augment: bool = True, num_workers: int = 0,
                       random_seed: int = 42) -> Tuple[DataLoader, DataLoader]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    
    Args:
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        sequence_length: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·ã•
        train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
        augment: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¡Œã†ã‹ã©ã†ã‹
        num_workers: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        
    Returns:
        train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        val_loader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    """
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_dataset = KeyboardIntentDataset(
        data_dir=data_dir,
        sequence_length=sequence_length,
        train=True,
        train_ratio=train_ratio,
        augment=augment,
        random_seed=random_seed
    )
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    val_dataset = KeyboardIntentDataset(
        data_dir=data_dir,
        sequence_length=sequence_length,
        train=False,
        train_ratio=train_ratio,
        augment=False,  # æ¤œè¨¼æ™‚ã¯æ‹¡å¼µãªã—
        random_seed=random_seed
    )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†")
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    
    return train_loader, val_loader


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚¹ãƒˆ
    data_dir = "data/training/user_001"
    
    if os.path.exists(data_dir):
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        dataset = KeyboardIntentDataset(data_dir, augment=True)
        
        # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã®è¡¨ç¤º
        info = dataset.get_sample_info()
        print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¡¨ç¤º
        class_weights = dataset.get_class_weights()
        print(f"ã‚¯ãƒ©ã‚¹é‡ã¿: {class_weights[:10]}...")  # æœ€åˆã®10å€‹
        
        # ã‚µãƒ³ãƒ—ãƒ«ã®å–å¾—ãƒ†ã‚¹ãƒˆ
        if len(dataset) > 0:
            features, label = dataset[0]
            print(f"ã‚µãƒ³ãƒ—ãƒ«0:")
            print(f"  ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")
            print(f"  ãƒ©ãƒ™ãƒ«: {label} ({dataset.index_to_key(label)})")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆãƒ†ã‚¹ãƒˆ
        train_loader, val_loader = create_data_loaders(data_dir, batch_size=16)
        
    else:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data_dir}")
        print("ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
